# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vZnUZlYJ2hThcHvpcqM_oOgV2NjOYoD7

# **Proyek Pertama: Menyelesaikan Permasalahan Human Resources Jaya Jaya Maju**

---

*   Nama : Faishal Anwar Hasyim
*   Email : anwarfaishal86@gmail.com
*   Id Dicoding : anwarfaishal86

# Persiapan

**Menyiapkan Library yang dibutuhkan**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import shap

from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, silhouette_samples
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier, StackingClassifier
from sklearn.feature_selection import RFE
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
from imblearn.over_sampling import SMOTE
from sklearn.metrics import confusion_matrix as sklearn_confusion_matrix # Import with alias
from xgboost import XGBClassifier

"""# Data Understanding"""

employees_df = pd.read_csv(
    "https://raw.githubusercontent.com/dicodingacademy/dicoding_dataset/main/employee/employee_data.csv",
    encoding='windows-1252'
)
employees_df.head(5)

employees_df.info()

employees_df.describe()

employees_df.isna().sum()

clean_df = employees_df
clean_df = clean_df.dropna()

clean_df.columns = ["employee_id",
                    "age"	,
                    "attrition",
                    "business_travel",
                    "daily_rate",
                    "department",
                    "distance_from_home",
                    "education",
                    "education_field",
                    "employee_count",
                    "environment_satisfaction",
                    "gender",
                    "hourly_rate",
                    "job_involvement",
                    "job_level",
                    "job_role",
                    "job_satisfaction",
                    "marital_status",
                    "monthly_income",
                    "monthly_rate",
                    "num_companies_worked",
                    "over18",
                    "over_time",
                    "percent_salary_hike",
                    "performance_rating",
                    "relationship_satisfaction",
                    "standard_hours",
                    "stock_option_level",
                    "total_working_years",
                    "training_times_last_year",
                    "work_life_balance",
                    "years_at_company",
                    "years_in_current_role",
                    "years_since_last_promotion",
                    "years_with_curr_manager"]
clean_df.head(5)

# Mengganti nilai 0 dengan 'No' dan 1 dengan 'Yes'
exploratory_df = clean_df.copy()

# Mapping untuk kolom yang relevan
attrition_map = {0: 'No', 1: 'Yes'}
education_map = {1: 'Below College', 2: 'College', 3: 'Bachelor', 4: 'Master', 5: 'Doctor'}
environment_satisfaction_map = {1: 'Low', 2: 'Medium', 3: 'High', 4: 'Very High'}
job_involvement_map = {1: 'Low', 2: 'Medium', 3: 'High', 4: 'Very High'}
job_satisfaction_map = {1: 'Low', 2: 'Medium', 3: 'High', 4: 'Very High'}
performance_rating_map = {1: 'Low', 2: 'Good', 3: 'Excellent', 4: 'Outstanding'}
relationship_satisfaction_map = {1: 'Low', 2: 'Medium', 3: 'High', 4: 'Very High'}
work_life_balance_map = {1: 'Low', 2: 'Good', 3: 'Excellent', 4: 'Outstanding'}

# Mengganti nilai dengan deskripsi
exploratory_df['attrition'] = exploratory_df['attrition'].replace(attrition_map)
exploratory_df['education'] = exploratory_df['education'].replace(education_map)
exploratory_df['environment_satisfaction'] = exploratory_df['environment_satisfaction'].replace(environment_satisfaction_map)
exploratory_df['job_involvement'] = exploratory_df['job_involvement'].replace(job_involvement_map)
exploratory_df['job_satisfaction'] = exploratory_df['job_satisfaction'].replace(job_satisfaction_map)
exploratory_df['performance_rating'] = exploratory_df['performance_rating'].replace(performance_rating_map)
exploratory_df['relationship_satisfaction'] = exploratory_df['relationship_satisfaction'].replace(relationship_satisfaction_map)
exploratory_df['work_life_balance'] = exploratory_df['work_life_balance'].replace(work_life_balance_map)

# Menampilkan DataFrame setelah penggantian
# Menyimpan DataFrame ke file CSV
exploratory_df.to_csv('exploratory_df.csv', index=False)

exploratory_df.head(5)

numerical = [
    "age", "daily_rate", "distance_from_home",
    "hourly_rate", "job_level", "monthly_income", "monthly_rate",
    "num_companies_worked", "percent_salary_hike",
    "standard_hours", "stock_option_level",
    "total_working_years", "training_times_last_year",
    "years_at_company", "years_in_current_role",
    "years_since_last_promotion", "years_with_curr_manager"
]

# Membuat histogram dalam frame yang berbeda
for feature in numerical:
    plt.figure(figsize=(10, 6))

    # Ambil nilai dari data
    data = exploratory_df[feature].dropna()

    # Hitung frekuensi dan bin
    counts, bins = np.histogram(data, bins=10)

    # Warna: merah untuk tertinggi, biru untuk lainnya
    colors = ['#ff9999' if count == max(counts) else '#66b3ff' for count in counts]

    # Gambar bar
    plt.bar(bins[:-1], counts, width=np.diff(bins), color=colors, edgecolor='black', align='edge')

    # Tambahkan label tengah bin ke sumbu X
    bin_centers = 0.5 * (bins[:-1] + bins[1:])
    plt.xticks(bin_centers, [f'{center:.0f}' for center in bin_centers], rotation=0)

    # Judul dan label
    plt.title(f'Histogram of {feature}', fontsize=16)
    plt.xlabel(feature, fontsize=14)
    plt.ylabel('Count', fontsize=14)
    plt.grid(axis='y', linestyle='--', alpha=0.7)

    plt.tight_layout()
    plt.show()

"""Dari visualisasi diatas kita bisa melihat rentang nilai untuk fitir numerik
*   grafik berwarna merah menunjukkan nilai tertinggi dari sebuah fitur numerik


"""

categorical = ["attrition", "business_travel", "department", "education", "education_field",
               "environment_satisfaction", "gender", "job_involvement", "job_level", "job_role",
               "job_satisfaction", "marital_status", "over18", "over_time", "performance_rating",
               "relationship_satisfaction", "work_life_balance"]

# Daftar pasangan warna (tua, muda)
color_pairs = [
    ('#1f77b4', '#aec7e8'),   # Biru
    ('#d62728', '#ff9896'),   # Merah
    ('#2ca02c', '#98df8a'),   # Hijau
    ('#9467bd', '#c5b0d5'),   # Ungu
    ('#ff7f0e', '#ffbb78'),   # Oranye
    ('#8c564b', '#c49c94'),   # Coklat
    ('#17becf', '#9edae5'),   # Cyan
    ('#e377c2', '#f7b6d2')    # Pink
]

# Loop untuk setiap fitur
for i, feature in enumerate(categorical):
    # Pilih warna berdasarkan urutan
    color_pair = color_pairs[i % len(color_pairs)]

    # Ambil kategori unik sesuai urutan pertama muncul
    categories = exploratory_df[feature].dropna().unique().tolist()

    # Hitung frekuensi dan kategori terbanyak
    counts = exploratory_df[feature].value_counts()
    max_count = counts.max()
    max_categories = counts[counts == max_count].index.tolist()

    # Buat warna: tua untuk paling banyak, muda untuk lainnya
    colors = [color_pair[0] if cat in max_categories else color_pair[1] for cat in categories]

    # Plot per fitur dalam frame baru
    plt.figure(figsize=(8, 5))
    sns.countplot(data=exploratory_df, x=feature, order=categories, palette=colors)

    plt.title(feature, fontsize=16)
    plt.xlabel(feature, fontsize=12)
    plt.ylabel('Count', fontsize=12)
    plt.xticks(rotation=0)
    plt.grid(axis='y', linestyle='--', alpha=0.6)
    sns.despine()

    plt.tight_layout()
    plt.show()

"""Dari visualisasi diatas kita bisa melihat rentang nilai untuk fitir kategori

*   grafik berwarna lebih tua menunjukkan nilai tertinggi dari sebuah fitur kategori
"""

def categorical_plot(features, df, segment_feature=None):
    fig, ax = plt.subplots(len(features), 1,figsize=(8,53))
    for i, feature in enumerate(features):
        if segment_feature:
            sns.countplot(data=df, x=segment_feature, hue=feature, ax=ax[i])
        else:
            sns.countplot(data=df, x=feature, ax=ax[i])
    plt.tight_layout()
    plt.show()

categorical_plot(
    features=["over_time", "business_travel", "department", "education_field", "education", "environment_satisfaction", "gender", "job_role", "job_involvement","job_satisfaction", "marital_status", "over18", "performance_rating", "relationship_satisfaction" , "work_life_balance"],
    df=exploratory_df,
    segment_feature="attrition"
)

"""Dari visualisasi, kita dapat melihat hubungan antara fitur-fitur kategorikal dengan fitur Attrition. Meskipun terlihat adanya perbedaan antara karyawan yang bertahan (Attrition = No) dan yang keluar (Attrition = Yes), sebagian besar fitur tidak menunjukkan pola yang dapat disimpulkan secara kuat karena distribusi datanya tidak merata. Namun, fitur OverTime menjadi pengecualian. Grafik menunjukkan bahwa meskipun jumlah karyawan yang melakukan lembur (OverTime = Yes) jauh lebih banyak dibandingkan yang tidak lembur, proporsi karyawan yang keluar dari perusahaan jauh lebih tinggi pada kelompok yang melakukan lembur tersebut. Hal ini mengindikasikan adanya korelasi antara lembur yang berlebihan dengan kemungkinan karyawan untuk resign."""

# Salin semua kolom yang diperlukan
df_corr = clean_df[[
    "age",
    "attrition",
    "business_travel",
    "daily_rate",
    "distance_from_home",
    "education",
    "employee_count",
    "environment_satisfaction",
    "gender",
    "hourly_rate",
    "job_involvement",
    "job_level",
    "job_satisfaction",
    "marital_status",
    "monthly_income",
    "monthly_rate",
    "num_companies_worked",
    "over18",
    "over_time",
    "percent_salary_hike",
    "performance_rating",
    "relationship_satisfaction",
    "standard_hours",
    "stock_option_level",
    "total_working_years",
    "training_times_last_year",
    "work_life_balance",
    "years_at_company",
    "years_in_current_role",
    "years_since_last_promotion",
    "years_with_curr_manager"
]].copy()

# Encoding kolom kategorikal yang bisa dikonversi secara langsung
df_corr['over_time'] = df_corr['over_time'].map({'Yes': 1, 'No': 0})
df_corr['gender'] = df_corr['gender'].map({'Male': 1, 'Female': 0})
df_corr['over18'] = df_corr['over18'].map({'Y': 1})  # jika hanya 1 nilai, bisa di-drop nantinya
df_corr['business_travel'] = df_corr['business_travel'].map({
    'Non-Travel': 0, 'Travel_Rarely': 1, 'Travel_Frequently': 2
})
df_corr['marital_status'] = df_corr['marital_status'].map({
    'Single': 0, 'Married': 1, 'Divorced': 2
})

df_corr.head()

# Asumsikan df adalah DataFrame kamu
# Drop kolom non-numerik atau yang tidak relevan untuk korelasi

# Hitung korelasi
corr_matrix = df_corr.corr()

# Buat heatmap
plt.figure(figsize=(16, 12))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap="coolwarm", linewidths=0.5)
plt.title("Heatmap Korelasi Antar Fitur")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""Dari visualisasi confusion matrix di atas, kita dapat menilai sejauh mana model mampu membedakan antara karyawan yang keluar (Attrition = Yes) dan yang tetap bekerja (Attrition = No). Nilai yang semakin mendekati 1 atau -1 akan memiliki hubungan yang lebih kuat. Meskipun confusion matrix tidak secara eksplisit menampilkan fitur-fitur yang paling berpengaruh, pola kesalahan prediksi yang terlihat—seperti jumlah false positives dan false negatives—dapat memberi petunjuk bahwa terdapat fitur-fitur tertentu yang memiliki pengaruh kuat terhadap prediksi Attrition. Analisis lebih lanjut, seperti feature importance, diperlukan untuk mengidentifikasi fitur-fitur tersebut secara spesifik.

# Data Preparation / Preprocessing
"""

# data model
from sklearn.preprocessing import LabelEncoder
df_model = clean_df.copy()
df_model = df_model.drop(columns=['employee_id'])
label_encoders = {}

for col in df_model.select_dtypes(include='object').columns:
    le = LabelEncoder()
    df_model[col] = le.fit_transform(df_model[col])
    label_encoders[col] = le

# Memisahkan feature dan target
X = df_model.drop(['attrition'], axis=1)
y = df_model['attrition']

# Step 2: Recursive Feature Elimination (RFE) for Feature Selection
rfe_selector = RFE(estimator=RandomForestClassifier(), n_features_to_select=10, step=1)
rfe_selector = rfe_selector.fit(X, y) # Fit RFE on the encoded data

# Select top features
selected_features = X.columns[rfe_selector.support_]
X_selected = X[selected_features]

X_selected

# Step 3: Handle Data Imbalance using SMOTE
smote = SMOTE(random_state=42)
X_balanced, y_balanced = smote.fit_resample(X_selected, y)

# Step 4: Split Data into Train and Test Sets
X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)
# X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)

# Summary of data preparation
data_summary = {
    "Selected Features": list(selected_features),
    "X_train Shape": X_train.shape,
    "X_test Shape": X_test.shape,
    "y_train Distribution": y_train.value_counts(),
    "y_test Distribution": y_test.value_counts()
}

data_summary

# split data untuk training dan testing
X_train, X_test, y_train, y_test = train_test_split(X_selected, y, stratify=y, test_size=0.2, random_state=42)

# Menerapkan SMOTE untuk mengatasi imbalance
from imblearn.combine import SMOTEENN
smote = SMOTEENN(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

# Menggabungkan kembali data
df_resampled = pd.concat([pd.DataFrame(X_train_res, columns=X_train.columns),
                          pd.Series(y_train_res, name='Attrition')], axis=1)

"""# Modeling"""

# Step 1: Logistic Regression (Baseline Model)
logistic_model = LogisticRegression(random_state=42, max_iter=1000)
logistic_model.fit(X_train, y_train)

# Predictions for Logistic Regression
y_pred_logistic = logistic_model.predict(X_test)

# Step 2: Random Forest (Complex Model)
rf_model = RandomForestClassifier(random_state=42, n_estimators=100)
rf_model.fit(X_train, y_train)

# Predictions for Random Forest
y_pred_rf = rf_model.predict(X_test)

# Step 3: XGBoost (Advanced Model)
xgb_model = XGBClassifier(random_state=42, eval_metric='logloss')
xgb_model.fit(X_train, y_train)

# Predictions for XGBoost
y_pred_xgb = xgb_model.predict(X_test)

# Step 4: Support Vector Machine (Advanced Model)
svm_model = SVC(random_state=42, kernel='rbf')
svm_model.fit(X_train, y_train)

# Predictions for SVM
y_pred_svm = svm_model.predict(X_test)

"""# Evaluation"""

# Metrics for Logistic Regression
logistic_accuracy = accuracy_score(y_test, y_pred_logistic)
logistic_precision = precision_score(y_test, y_pred_logistic)
logistic_recall = recall_score(y_test, y_pred_logistic)
logistic_f1 = f1_score(y_test, y_pred_logistic)

# Metrics for Random Forest
rf_accuracy = accuracy_score(y_test, y_pred_rf)
rf_precision = precision_score(y_test, y_pred_rf)
rf_recall = recall_score(y_test, y_pred_rf)
rf_f1 = f1_score(y_test, y_pred_rf)

# Metrics for XGBoost
xgb_accuracy = accuracy_score(y_test, y_pred_xgb)
xgb_precision = precision_score(y_test, y_pred_xgb)
xgb_recall = recall_score(y_test, y_pred_xgb)
xgb_f1 = f1_score(y_test, y_pred_xgb)

# Metrics for SVM
svm_accuracy = accuracy_score(y_test, y_pred_svm)
svm_precision = precision_score(y_test, y_pred_svm)
svm_recall = recall_score(y_test, y_pred_svm)
svm_f1 = f1_score(y_test, y_pred_svm)

# Comparison Table
model_comparison = pd.DataFrame({
    "Model": ["Logistic Regression", "Random Forest", "XGBoost", "SVM"],
    "Accuracy": [logistic_accuracy, rf_accuracy, xgb_accuracy, svm_accuracy],
    "Precision": [logistic_precision, rf_precision, xgb_precision, svm_precision],
    "Recall": [logistic_recall, rf_recall, xgb_recall, svm_recall],
    "F1-Score": [logistic_f1, rf_f1, xgb_f1, svm_f1]
})

model_comparison

# Detailed Classification Reports
print("\nClassification Report: Logistic Regression")
print(classification_report(y_test, y_pred_logistic))

print("\nClassification Report: Random Forest")
print(classification_report(y_test, y_pred_rf))

print("\nClassification Report: XGBoost")
print(classification_report(y_test, y_pred_xgb))

print("\nClassification Report: SVM")
print(classification_report(y_test, y_pred_svm))

# Confusion Matrix Visualization for XGBoost
xgb_confusion_matrix = sklearn_confusion_matrix(y_test, y_pred_xgb)
plt.figure(figsize=(6, 4))
sns.heatmap(xgb_confusion_matrix, annot=True, fmt="d", cmap="Greens")
plt.title("Confusion Matrix: XGBoost")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# Simpan model
joblib.dump(xgb_model, 'xgb_attrition_model.pkl')

!pip freeze > requirements.txt

!cat requirements.txt

